{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '\"\\n\\n Metal revert \\n\\nI see you reverted one of my edits to the metal article. You say that the sentence is referring to metals in general, however I disagree. The paragraph starts off: \"\"The transition metals...\"\", so really the whole paragraph is about transitional metals, of which steel is not. As such, I would like my edit reinstated.   \"'\n",
      " 'REDIRECT Talk:Georgian keyboard layout'\n",
      " '\"\\n\\n:Image:Angelg2.png has been listed for deletion\\n\\n An image or media file you uploaded, Image:Angelg2.png, has been listed at Wikipedia:Images and media for deletion. Please look there to see why this is (you may have to search for the title of the image to find its entry), if you are interested in it not being deleted. Thank you.\"'\n",
      " ...,\n",
      " '\"\\n\\n Hi Bbb23 I need your help and some advice about possible sockpuppetry \\n\\nHi Bbb23,\\n\\nI need your help and some advice about possible sockpuppetry. I am also going to ping User:PBS  @PBS so that he may have advice as well, or if you are really busy just now.\\n\\nThere has been ongoing disputes, edit warring, blatant blp issues, NPA\\'s up to like 3 now from one certain editor.in and around the article for a pianist here at en wikipedia, as well as the blp board, edit warring boards (reports by two  different editors about the same James Rhodes (pianist)  article, page protection board, where @CambridgeBayWeather saw all the blp issuesthat were going on at the time she judged the need for protection there at the pianist article. User:CambridgeBayWeather in her admin role semi-protected the article for one week, then there were three WP:NPA violations against him, which I warned the editor on his talk page on two occasions.\\n\\n Since then the editor has struck up a whole band of possible sock monkeys to help defend him, and also some of the ones have made personal attacks againt me as well in the ani the editor filed.  I have been trying to state what has actually happened over this while, but there is so much disruption, attacking, and lacking good faith by him and some others there.\\n\\nI just got to thinking this is so weird. All the possible soccy editors seemed to be acting in concert in how the flow of things went there so far.  I just kept my thoughts about all this to myself until now, but it occurred to me to check the block log of this editor to see if he had ever had any trouble with issues that he may have been blocked for.  I  asked him about the two blocks listed in the report. One block was for 72 hours and the next block was an INdeffinent. block. I noticed the offenses listed was sock puppeting and abusing multiple accounts. \\n\\nWhen I asked him about the sockpuppetry charge in the ani, he said something to the effect that \"\"he was testing wikipedias security\"\" by \"\"setting up some vandalism only accounts\"\" to test security. You could read his exact storyline that he posted on there. Are editors allowed to \"\"test security\"\" by making up sockpuppets to vandalize editors, articles and such? Is he really working for you as a clerk or something?\\n\\nTo paraphrase, he said at ani it was just a couple of accounts, maybe 2 just to test with. I took his word on that for awhile, but then I took a break from making new articles, to just try typing his editor name and sockpuppet in wp search. This is what I found, it had your name on it so I thought you would be the perfect person to explain to me how this works exactly. [title=Wikipedia:Sockpuppet_investigations/Gabucho181/Archive&oldid;=663847791]\\n\\nMaybe I am not reading the pages and archives correctly since I do not follow the spi reports very often.  I see people speak of a duck rule, and or diffs of evidence is needed to request an investigation. I do not know myself is all this meets the duck rule or if there is any real evidence that this user may be continuing to abuse multi accounts or not,  But I was stunned to se reports on his  archive go back to januaey and the latest one was around may 18. Could you read this over if you have time to see if it warrants asking for all this to be checked on. I do not know how to do a spi report. I am not sure if it can be added on to the ongoing list that goes back several months? Please look it over and let me know something. I am going to work offline on my new article sets tonight, and upload them tomorrow or the next day. I have been doing a series of articles about different writers around the world, and then I make a few stubs from red links in the one about the writer. \\n\\nThis is all very discouraging.   \\nThank you.\\n\\nCheers!       \\n\\n\"'\n",
      " 'germany took over most of it and'\n",
      " 'REDIRECT User talk:Orphan Wiki/Archive 2']\n"
     ]
    }
   ],
   "source": [
    "db = pd.read_csv(\"train.csv\")\n",
    "db = db.reindex(np.random.permutation(db.index))\n",
    "comments = db['comment_text'].as_matrix()\n",
    "print (comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "stop_words.append('')\n",
    "\n",
    "for x in range(ord('b'), ord('z')+1):\n",
    "    stop_words.append(chr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', '', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~0123456789\n",
      "REDIRECT Talk:Georgian keyboard layout\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation_edit = string.punctuation.replace('\\'','') +\"0123456789\"\n",
    "print ((punctuation_edit))\n",
    "print (comments[1])\n",
    "outtab = \"                                         \"\n",
    "trantab = str.maketrans(punctuation_edit, outtab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/nupur/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(comments)):\n",
    "# for i in range(5):\n",
    "    comments[i] = comments[i].lower().translate(trantab)\n",
    "    l = []\n",
    "    for word in comments[i].split():\n",
    "        l.append(stemmer.stem(lemmatiser.lemmatize(word,pos=\"v\")))\n",
    "    comments[i] = \" \".join(l)\n",
    "#     result_words[i] = re.split(' |\\n',result_words[i])\n",
    "#     result_words[i] = [word for word in result_words[i] if word not in stop_words]\n",
    "# print(lemmatiser.lemmatize(\"having\",pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(stop_words=stop_words)\n",
    "tf = count_vector.fit_transform(comments).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 102434)\n"
     ]
    }
   ],
   "source": [
    "# print(count_vector.get_feature_names())\n",
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = db[['toxic', 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']].as_matrix()\n",
    "def shuffle(matrix, target, test_proportion):\n",
    "    ratio = int(matrix.shape[0]/test_proportion)\n",
    "    X_train = matrix[ratio:,:]\n",
    "    X_test =  matrix[:ratio,:]\n",
    "    Y_train = target[ratio:,:]\n",
    "    Y_test =  target[:ratio,:]\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = shuffle(tf, labels, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = ['toxic' , 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']\n",
    "# labels = ['toxic']\n",
    "#  models = ['linear' , 'rbf']\n",
    "models = ['linear']\n",
    "datalabels = []\n",
    "clf = [[]]\n",
    "\n",
    "abc = svm.SVC(kernel='linear')\n",
    "abc.fit(tf,db['toxic'].as_matrix())\n",
    "\n",
    "# for ix in range(len(labels)):\n",
    "#     datalabels.append(X_train[labels[ix]])\n",
    "#     for i in range(len(models)):\n",
    "#         clf[ix].append(svm.SVC(kernel=models[i]))\n",
    "#         clf[ix][i].fit(tf,datalabels[ix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hamm():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
