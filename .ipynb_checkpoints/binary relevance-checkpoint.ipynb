{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"Brontothere\\nThe brontothere was called both brontotherium and the other one I don't know.\"\n",
      " '\"\\nI didn\\'t believe that article when I saw it. It\\'s not the best evidence. 72.81.208.117  \\n\\n Suggest Ruff Ruffman be moved here \\n\\nI suggest that we merge the article Ruff Ruffman into his section at FETCH! with Ruff Ruffman. It is almost all identical to the information in this artcle anyway Peb1991 \\n\\nI second the motion, should redirect the article to here.  \\n\\nOkay, I set up the redirect. I am not familiar with the process for deleting articles like that, so the the original content is still on the Ruff Ruffman page. Can someone please show me the process for this. Peb1991 \\n\\n Auditions on website \\n\\nThe next auditions have been posted on the Fetch website. Someone should get the information off there. —The preceding unsigned comment was added by 24.195.133.53  .\\n\\n Links to Contestants \\n\\nI removed the links to the contestants names. Their pages are in the process of deletion anyway. Does any one think character descriptions should be added to this page, make a character page for all of them, or none at all? Peb1991 \\n\\n Season 2 Episodes \\n\\nAnother thing, TV.com lists different episodes than the current page does. Which is correct?, and where does TV.com and us (Wikipedia) get  these  descriptions from? (and which is reliable?) Peb1991 \\nOK, here are the ones from PBS KIDS http://pbskids.org/fetch/parentsteachers/program/episodes.html\\nI put in the ones from PBSKIDS (obviously the most accurate ) ) As far as my station has it listed, the season 2 episodes seem to go 4 new ones a week, with a rerun one Friday (like season 1).\\n\\nFair use rationale for Image:Fetch! with ruff ruffman.jpg\\n\\n:Image:Fetch! with ruff ruffman.jpg is being used on this article. I notice the image page specifies that the image is being used under fair use but there is no explanation or rationale as to why its use in Wikipedia articles constitutes fair use. In addition to the boilerplate fair use template, you must also write out on the image description page a specific explanation or rationale for why using this image in each article is consistent with fair use.\\n\\nPlease go to the image description page and edit it to include a fair use rationale. Using one of the templates at Wikipedia:Fair use rationale guideline is an easy way to insure that your image is in compliance with Wikipedia policy, but remember that you must complete the template. Do not simply insert a blank template on an image page.\\n\\nIf there is other other fair use media, consider checking that you have specified the fair use rationale on the other images used on this page.  Note that any fair use images uploaded after 4 May, 2006, and lacking such an explanation will be deleted one week after they have been uploaded, as described on criteria for speedy deletion. If you have any questions please ask them at the Media copyright questions page. Thank you. \\n\\n Episode Articles for Deletion \\n\\nFYI, a proposal to delete all FETCH! episode articles has been made here\\n \\n\\n Season 3? \\n\\nIs there proof of season 3 airing winter 2008? I haven\\'t seen anything.  \\nI THINK IT IS AIRING THIS SEPTEMBER. HOWEVER, THE KIDS WILL NOT HAVE TIME TO WATCH IT BECAUSE THEY WILL BE BACK AT SCHOOL AT THE TIME. SO EITHER THEY AIR SEASON 3 OF FETCH WITH RUFF RUFFMAN TOMORROW, OR THEY ARE GOING TO LOSE A LOT OF VIEWERS. IT WILL BE CANCELLED.  —Preceding unsigned comment added by [[Special:Contrib'\n",
      " 'Artivist Awards \\n http://www.graphic-design.com/gallery/artivist-announces-celebrity-honorees-2013-artivist-awards\\n http://www.latinheat.com/the-biz/10th-anniversary-artivist-awards-honorees-announced/'\n",
      " ..., 'Why not? Where do I add this freaking text? Try again then'\n",
      " \"Seaforth/Kenny Everett\\nI notice you and your cronie Kitchen Knife are being smart arses again by changing edits on Liverpool,,,,furthermore because I happen to be linked with edits I have made. In this instance Seaforth maybe in Sefton that is not questioned. Kenny Everett was on a documentary a few months ago of how fond he was of John Lennon as they were all 'from Liverpool'. Your intense scrutiny over these borderline districts is pathetic. Should someone else come along in the future and challenge you I hope you are not as much a pratt as you are here.\"\n",
      " 'No, there is no consensus for this.  This section is a discussion of our coverage of weight budgets and discussion of dab and the like is tangential to this.']\n"
     ]
    }
   ],
   "source": [
    "db = pd.read_csv(\"train.csv\")\n",
    "db = db.reindex(np.random.permutation(db.index))\n",
    "comments = db['comment_text'].as_matrix()\n",
    "print (comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "stop_words.append('')\n",
    "\n",
    "for x in range(ord('b'), ord('z')+1):\n",
    "    stop_words.append(chr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', '', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~0123456789\n",
      "\"\n",
      "I didn't believe that article when I saw it. It's not the best evidence. 72.81.208.117  \n",
      "\n",
      " Suggest Ruff Ruffman be moved here \n",
      "\n",
      "I suggest that we merge the article Ruff Ruffman into his section at FETCH! with Ruff Ruffman. It is almost all identical to the information in this artcle anyway Peb1991 \n",
      "\n",
      "I second the motion, should redirect the article to here.  \n",
      "\n",
      "Okay, I set up the redirect. I am not familiar with the process for deleting articles like that, so the the original content is still on the Ruff Ruffman page. Can someone please show me the process for this. Peb1991 \n",
      "\n",
      " Auditions on website \n",
      "\n",
      "The next auditions have been posted on the Fetch website. Someone should get the information off there. —The preceding unsigned comment was added by 24.195.133.53  .\n",
      "\n",
      " Links to Contestants \n",
      "\n",
      "I removed the links to the contestants names. Their pages are in the process of deletion anyway. Does any one think character descriptions should be added to this page, make a character page for all of them, or none at all? Peb1991 \n",
      "\n",
      " Season 2 Episodes \n",
      "\n",
      "Another thing, TV.com lists different episodes than the current page does. Which is correct?, and where does TV.com and us (Wikipedia) get  these  descriptions from? (and which is reliable?) Peb1991 \n",
      "OK, here are the ones from PBS KIDS http://pbskids.org/fetch/parentsteachers/program/episodes.html\n",
      "I put in the ones from PBSKIDS (obviously the most accurate ) ) As far as my station has it listed, the season 2 episodes seem to go 4 new ones a week, with a rerun one Friday (like season 1).\n",
      "\n",
      "Fair use rationale for Image:Fetch! with ruff ruffman.jpg\n",
      "\n",
      ":Image:Fetch! with ruff ruffman.jpg is being used on this article. I notice the image page specifies that the image is being used under fair use but there is no explanation or rationale as to why its use in Wikipedia articles constitutes fair use. In addition to the boilerplate fair use template, you must also write out on the image description page a specific explanation or rationale for why using this image in each article is consistent with fair use.\n",
      "\n",
      "Please go to the image description page and edit it to include a fair use rationale. Using one of the templates at Wikipedia:Fair use rationale guideline is an easy way to insure that your image is in compliance with Wikipedia policy, but remember that you must complete the template. Do not simply insert a blank template on an image page.\n",
      "\n",
      "If there is other other fair use media, consider checking that you have specified the fair use rationale on the other images used on this page.  Note that any fair use images uploaded after 4 May, 2006, and lacking such an explanation will be deleted one week after they have been uploaded, as described on criteria for speedy deletion. If you have any questions please ask them at the Media copyright questions page. Thank you. \n",
      "\n",
      " Episode Articles for Deletion \n",
      "\n",
      "FYI, a proposal to delete all FETCH! episode articles has been made here\n",
      " \n",
      "\n",
      " Season 3? \n",
      "\n",
      "Is there proof of season 3 airing winter 2008? I haven't seen anything.  \n",
      "I THINK IT IS AIRING THIS SEPTEMBER. HOWEVER, THE KIDS WILL NOT HAVE TIME TO WATCH IT BECAUSE THEY WILL BE BACK AT SCHOOL AT THE TIME. SO EITHER THEY AIR SEASON 3 OF FETCH WITH RUFF RUFFMAN TOMORROW, OR THEY ARE GOING TO LOSE A LOT OF VIEWERS. IT WILL BE CANCELLED.  —Preceding unsigned comment added by [[Special:Contrib\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation_edit = string.punctuation.replace('\\'','') +\"0123456789\"\n",
    "print ((punctuation_edit))\n",
    "print (comments[1])\n",
    "outtab = \"                                         \"\n",
    "trantab = str.maketrans(punctuation_edit, outtab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/nupur/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(comments)):\n",
    "# for i in range(5):\n",
    "    comments[i] = comments[i].lower().translate(trantab)\n",
    "    l = []\n",
    "    for word in comments[i].split():\n",
    "        l.append(stemmer.stem(lemmatiser.lemmatize(word,pos=\"v\")))\n",
    "    comments[i] = \" \".join(l)\n",
    "#     result_words[i] = re.split(' |\\n',result_words[i])\n",
    "#     result_words[i] = [word for word in result_words[i] if word not in stop_words]\n",
    "# print(lemmatiser.lemmatize(\"having\",pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(stop_words=stop_words)\n",
    "tf = count_vector.fit_transform(comments).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 102434)\n"
     ]
    }
   ],
   "source": [
    "# print(count_vector.get_feature_names())\n",
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = db[['toxic', 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']].as_matrix()\n",
    "def shuffle(matrix, target, test_proportion):\n",
    "    ratio = int(matrix.shape[0]/test_proportion)\n",
    "    X_train = matrix[ratio:,:]\n",
    "    X_test =  matrix[:ratio,:]\n",
    "    Y_train = target[ratio:,:]\n",
    "    Y_test =  target[:ratio,:]\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = shuffle(tf, labels, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels = ['toxic' , 'severe_toxic' , 'obscene' , 'threat' , 'insult' , 'identity_hate']\n",
    "# labels = ['toxic']\n",
    "#  models = ['linear' , 'rbf']\n",
    "models = ['linear']\n",
    "datalabels = []\n",
    "clf = [[]]\n",
    "\n",
    "abc = svm.SVC(kernel='linear')\n",
    "abc.fit(tf,db['toxic'].as_matrix())\n",
    "\n",
    "# for ix in range(len(labels)):\n",
    "#     datalabels.append(X_train[labels[ix]])\n",
    "#     for i in range(len(models)):\n",
    "#         clf[ix].append(svm.SVC(kernel=models[i]))\n",
    "#         clf[ix][i].fit(tf,datalabels[ix])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
